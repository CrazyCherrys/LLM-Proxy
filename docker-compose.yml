version: '3.8'

services:
  llm-proxy:
    build: .
    container_name: llm-proxy
    ports:
      - "${PORT:-80}:80"
    environment:
      - TARGET_API=${TARGET_API:-https://vg.v1api.cc}
      - WORKER_PROCESSES=${WORKER_PROCESSES:-auto}
    volumes:
      - ./logs:/var/log/nginx
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/"]
      interval: 30s
      timeout: 10s
      retries: 3 